---
title: "hate_speech_estm"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
library(tidyverse)
library(stm)

setwd("/Users/Tom/Desktop/Projects/icwsm_data_challenge")
data <- read.csv('race_classifier/data/founta_race_annotated_2.csv') 
data <- data[!(data$label == "spam"),]
data <- data[!(data$afam == -9),]
data <- droplevels(data)
# 85708 observations remaining
data = data %>% distinct(cleaned_text, .keep_all = TRUE) # dropping duplicates, keeping first. .keep_all pertains to variables
# 75,029 observatons remaining, so over 10k duplicates
d = data %>% select(cleaned_text, label, afam)
# TODO: Check no odd special characters are getting through.
d$label = d$label %>% fct_collapse(abusive = c("hateful", "abusive"))
d$afam.maj <- ifelse(d$afam>0.5, 1, 0) # Consider using this as a covariate for content (it can only take binary values)
```

## Preprocessing

```{r preprocess, echo=TRUE}
processed <- textProcessor(d$cleaned_text, metadata = d)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thres = 5)
```


## Estimating k


```{r searchk, echo=TRUE}
# Commented out so do not run accidentally
#set.seed(14850)
# First trying the Lee-Mimno algorithm without any specified k.
# This failed whenever I had covariates so I omitted them
#kresult <- searchK(docs, vocab, K = 0,
#                   N = 1000, init.type = "Spectral", cores = 7, data = meta)

# OK, that didn't work well... it picked 97 topics. I think we should stick with something between 15-30. Will run the algorithm again tomorrow.
```


```{r searchkk, echo=TRUE}
# Next trying several different values of k
set.seed(14850)
kresult.2 <- searchK(out$documents, out$vocab, K = c(10,20,30,40,50,60), N = 5000, 
                   prevalence =~ label * s(afam), cores = 7, data = out$meta)
plot(kresult.2)
# Exclusivity does not work with content covariates. Removed the content covariate from the searchK function.
```

## Estimating an initial model to check everything is working

```{r model, echo=TRUE}
K=30
fit <- stm(documents = out$documents, vocab = out$vocab, K=K, 
           prevalence =~ label * s(afam),
           max.em.its = 100, data = out$meta, init.type = "Spectral", verbose = TRUE
           )
# ngroups splits the data, it may speed things up although the documentation notes that it is somewhat experimental
# I have removed the content covariate. I think prevalence should be enough here.

# Notes on previous models
# Model with 75 em steps terminates before convergencE (only data.label as prevalence, no content covariates)
# The model with  K=20, prevalence =~ data.label + s(data.white), content =~ data.label converged after 15 iterations.
# Now I'm running a model with an interaction between label and prob_afam. It too 22 iterations.
```

## Storing model
```{r  store model, echo=TRUE}
# Commented out so doesn't re-run accidentally
#setwd("/Users/Tom/Desktop/Projects/icwsm_data_challenge/Rimages")
#save.image(file = "30topicimage.RData")
```

```{R load model, echo=TRUE}
# Now only need to run these lines.
setwd("/Users/Tom/Desktop/Projects/icwsm_data_challenge")
load(file = "Rimages/30topicimage_copy.RData")
```

## Analyzing the initial model
```{r initial analysis, echo=TRUE}
# Getting texts minus those dropped by text processing
removed <- c(out$docs.removed,processed$docs.removed)
for (t in 1:K) {
thoughts <- findThoughts(fit, texts =as.character(d[-removed,]$cleaned_text), topic=t, n = 10)
print(labelTopics(fit, topics=t, n=10))
print('\n')
print(thoughts)
print('\n')
}
# TODO: Something weird is happening with plot quote. it is showing a list object rather than the document itself. It is still usable but not as a plot

# Plotting topic proportions
plot(fit, type = "summary", xlim = c(0, .3))
# Looking at words by topic
labelTopics(fit)
# Plotting correlation
out.corr <- topicCorr(fit)
plot(out.corr)
# TODO: Can I colour these topics by association with AAE / abuse? This would then allow us to see if there are clusters of topics that are racialized / and abusive
```


## Plotting effect of covariate on topic prevalence
```{r prev plot, echo=TRUE}
predict_topics<- estimateEffect(formula = 1:K ~ label * afam, stmobj = fit, metadata = out$meta, uncertainty = "Global")
summary(predict_topics, topics=c(1,2)) # Summary for the first two topics
# This allows us to see prevalence of different topics in hateful versus normal tweets
plot(predict_topics, covariate = "label", topics = c(1:K),
 model = fit, method = "difference",
 cov.value1 = "abusive", cov.value2 = "normal",
 xlab = "More Abusive ... More Normal",
 main = "Effect of Abusive vs. Normal",
 xlim = c(-.05, .05), labeltype = "custom")

# No need for a separate predict
# Now the effect of AFAM
#predict_topics.afam<- estimateEffect(formula = 1:20 ~ afam, stmobj = fit, metadata = out$meta, uncertainty = "Global")
#summary(predict_topics.afam, topics=c(1,2)) # Summary for the first two topics

# This allows us to see expected proportion of a topic given the probability it is written in AAE. This will allow us to identify topics that are high in AAE usage.
plot(predict_topics, covariate = "afam", topics=4,
 model = fit, method = "continuous",  printlegend=FALSE,
 xaxt = "n",
 xlab = "Prob. AAE",
 main = "Effect of AAE for Topic 4",
labeltype = "custom")


### Here is a more complex plot where I allow the slope to vary for type
#prep <- estimateEffect(1:20 ~ label * afam, fit, metadata = out$meta,
#                       uncertainty = "Global")
#par(mfrow=c(2,1))
for (t in 1:30) {
#plot(predict_topics, covariate = "afam", topics=t, model = fit, method = "continuous", xlab = "Prob. #Afam", moderator = "label", moderator.value = "hateful", linecol = "blue", ylim = c(0, .12), printlegend = F)
#par(new=TRUE)
plot(predict_topics, covariate = "afam", topics = t, model = fit, method = "continuous", xlab = "Prob. AAE", moderator = "label", moderator.value = "abusive", linecol = "red", ylim = c(0, .6), printlegend = F)
par(new=TRUE)
plot(predict_topics, covariate = "afam", topics = t, model = fit, method = "continuous", xlab = "Prob. AAE", moderator = "label", moderator.value = "normal", linecol = "blue", ylim = c(0, .6), printlegend = F, main = paste("Topic", as.character(t), sep = " ") )
legend(x="topleft", y = 0.05, c("abusive", "normal"), lwd = 3, col = c("red", "blue"))
}
# This plot shows, for a given topic, the expect proportion of the topic by the probability the tweet is written in AAE by the hate classification of tweets. This will be useful once I have identified topics of interest.
# TODO: Pick 4 topics and make a plot showing different trends for all of them. e.g. four versions of this plot in a faceted grid.
```

```{r content, echo=TRUE}
# Now how does topic content vary?
plot(fit, type="perspectives", topics=c(4,20))
#plot(fit, type="perspectives", topics = 4) # This only works if there is a content covariate included.
```