---
title: "hate_speech_estm"
output:
  html_document: default
  pdf_document: default
---

# Training an STM on the Founta et al. dataset

## Instructions

If you want to load the pretrained model used in the analysis then skip to the cell called load model. If you want to train the model yourself, which takes around one hour on a 2.2GHz Intel Core i7 2014 MacBook Pro with 16GB RAM, then run all cells in the notebook (although the searchk cell is optional). 

### Loading the data
Note you must change the path to the correct path for the respository on your machine.
```{r setup, include=FALSE}
library(tidyverse)
library(stm)

setwd("/Users/Tom/Desktop/Projects/icwsm_data_challenge") # Change path to the correct path for the icwsm_data_challenge github repository
data <- read.csv('race_classifier/data/founta_race_annotated_2.csv') 
data <- data[!(data$label == "spam"),]
data <- data[!(data$afam == -9),]
data <- droplevels(data)
# 85708 observations remaining
data = data %>% distinct(cleaned_text, .keep_all = TRUE) # dropping duplicates, keeping first. .keep_all pertains to variables
# 75,029 observatons remaining, so over 10k duplicates
d = data %>% select(cleaned_text, label, afam)
# TODO: Check no odd special characters are getting through.
d$label = d$label %>% fct_collapse(abusive = c("hateful", "abusive"))
d$afam.maj <- ifelse(d$afam>0.5, 1, 0) # Consider using this as a covariate for content (it can only take binary values)
```

### Preprocessing
This code reads in the text data and runs basic preprocessing. Stop words, punctuation, and numbers are removed. All text is set to lower case. Terms that occur in fewer than 5 tweets are dropped, as are tweets that do not contain any valid terms.
```{r preprocess, echo=TRUE}
processed <- textProcessor(d$cleaned_text, metadata = d)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta, lower.thres = 5)
```


### Estimating k
This cell can be used to estimate models with different values of K and to compare them on various metrics (see documentation). Note that this takes several hours to run. It is not necessary to run this cell.
```{r searchk, echo=TRUE}
# Next trying several different values of k
# Note that this is currently set to run in parallel on 7 cores. Please change this variable if you want to run on a different number of cores or remove it to stop parallel computing.
set.seed(14850)
kresult.2 <- searchK(out$documents, out$vocab, K = c(10,20,30,40,50,60), N = 5000, 
                   prevalence =~ label * s(afam), cores = 7, data = out$meta)
plot(kresult.2)
# Exclusivity does not work with content covariates. Removed the content covariate from the searchK function.
```

### Estimating the STM with K=30

```{r model, echo=TRUE}
K=30
fit <- stm(documents = out$documents, vocab = out$vocab, K=K, 
           prevalence =~ label * s(afam),
           max.em.its = 100, data = out$meta, init.type = "Spectral", verbose = TRUE
           )
```

### Storing model 
Uncomment these lines if you wish to store the image of the workspace now that the model is trained. This is useful as it means you do not have to retrain the model if RStudio crashes.
```{r  store model, echo=TRUE}
#setwd("/Users/Tom/Desktop/Projects/icwsm_data_challenge/Rimages")
#save.image(file = "30topicimage.RData")
```

### Loading pretrained model.
You must download the RData image containing the trained model from Google Drive https://drive.google.com/file/d/1jHOwDG8cpC4vL7gDzk65uGz4voTaUL0p/view?usp=sharing and store the file in a folder called ``Rimages'' within the respository. The following cell can then be run to load the image, this contains all of the files output by model training (although it does not contain the results of searckK, which was run separately.)

```{R load model, echo=TRUE}
# Uncomment lines and run to load model
#setwd("/Users/Tom/Desktop/Projects/icwsm_data_challenge")
#load(file = "Rimages/30topicimage_copy.RData")
```

### Analyzing the initial model
This cell does three things:
  - For each topic, it prints out the top 10 words, according to various metrics, and the top 10 tweets with the highest topic proportion.
  - It plots the topic proportions, showing the proportion of documents across the corpus composed of each topic.
  - It plots the correlation network between topics.
```{r initial analysis, echo=TRUE}
# Getting texts minus those dropped by text processing
removed <- c(out$docs.removed,processed$docs.removed)
for (t in 1:K) {
thoughts <- findThoughts(fit, texts =as.character(d[-removed,]$cleaned_text), topic=t, n = 10)
print(labelTopics(fit, topics=t, n=10))
print('\n')
print(thoughts)
print('\n')
}
# TODO: Something weird is happening with plot quote. it is showing a list object rather than the document itself. It is still usable but not as a plot

# Plotting topic proportions
plot(fit, type = "summary", xlim = c(0, .3))
# Looking at words by topic
labelTopics(fit)
# Plotting correlation
out.corr <- topicCorr(fit)
plot(out.corr)
# TODO: Can I colour these topics by association with AAE / abuse? This would then allow us to see if there are clusters of topics that are racialized / and abusive
```


### Plotting effect of covariate on topic prevalence
This code makes two different plots:
  - Plots the prevalence of different topics by abusiveness.
  - Plots the change in topic proportion over the probability a tweet is written in AAE by abusive or normal for all 30 topics.
```{r prev plot, echo=TRUE}
predict_topics<- estimateEffect(formula = 1:K ~ label * afam, stmobj = fit, metadata = out$meta, uncertainty = "Global")
summary(predict_topics, topics=c(1,2)) # Summary for the first two topics
# This allows us to see prevalence of different topics in hateful versus normal tweets
plot(predict_topics, covariate = "label", topics = c(1:K),
 model = fit, method = "difference",
 cov.value1 = "abusive", cov.value2 = "normal",
 xlab = "More Abusive ... More Normal",
 main = "Effect of Abusive vs. Normal",
 xlim = c(-.05, .05), labeltype = "custom")


for (t in 1:30) {
plot(predict_topics, covariate = "afam", topics = t, model = fit, method = "continuous", xlab = "Prob. AAE", moderator = "label", moderator.value = "abusive", linecol = "red", ylim = c(0, .6), printlegend = F)
par(new=TRUE)
plot(predict_topics, covariate = "afam", topics = t, model = fit, method = "continuous", xlab = "Prob. AAE", moderator = "label", moderator.value = "normal", linecol = "blue", ylim = c(0, .6), printlegend = F, main = paste("Topic", as.character(t), sep = " ") )
legend(x="topleft", y = 0.05, c("abusive", "normal"), lwd = 3, col = c("red", "blue"))
}
```

### Plot topic differences
This code allows us to see how the top words differ for topics. I do not think it is vary useful in this case. If a content covariate is added to the model then we can see how a topic content varies by the covariate but cannot do so in this case since the model is trained without content covariates.
```{r content, echo=TRUE}
# Now how does topic content vary?
plot(fit, type="perspectives", topics=c(4,20))
#plot(fit, type="perspectives", topics = 4) # This only works if there is a content covariate included.
```